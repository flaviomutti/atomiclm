{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32efa2e4-99ce-4953-b1df-a7f83f5b5567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "from atomiclm.tokenizer import BasicTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfg-cell-0001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config ---\n",
    "DATA_PATH = '../data/the-verdict.txt'  # UTF-8 text file to train on (not in repo)\n",
    "VOCAB_SIZE = 276                       # small vocab for readable output\n",
    "SPECIAL_TOKENS = {\n",
    "    '<|endoftext|>': VOCAB_SIZE,       # registered after training, ID = next after vocab\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-train-0001",
   "metadata": {},
   "source": [
    "## Train\n",
    "Train a small BPE tokenizer so the merge steps are easy to inspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-cell-0001",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "tokenizer = BasicTokenizer()\n",
    "tokenizer.train(text, VOCAB_SIZE, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-special-0001",
   "metadata": {},
   "source": [
    "## Register special tokens\n",
    "Special tokens are literal strings that bypass BPE entirely.\n",
    "The encoder splits the input on these strings first, encodes the surrounding text\n",
    "with BPE, and emits the fixed ID for each special token match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e30a2f-9b6d-4271-a7c1-368c9f809b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.register_special_tokens(SPECIAL_TOKENS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-split-0001",
   "metadata": {},
   "source": [
    "## How the split works\n",
    "Before encoding, the text is split using a regex built from the special token strings.\n",
    "Each special token becomes its own chunk; everything else is encoded with BPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-cell-0001",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 'hello world<|endoftext|>second document'\n",
    "\n",
    "# This is what the encoder does internally before BPE.\n",
    "pattern = '(' + '|'.join(re.escape(k) for k in SPECIAL_TOKENS) + ')'\n",
    "chunks = re.split(pattern, sample)\n",
    "\n",
    "print('split pattern:', pattern)\n",
    "print('chunks       :', chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-encode-0001",
   "metadata": {},
   "source": [
    "## Encode\n",
    "Pass `allowed_special=\"all\"` to permit special tokens in the input.\n",
    "By default they are rejected to prevent accidental injection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327a1c48-3251-4a09-a627-ba6405e103f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer.encode(sample, allowed_special='all')\n",
    "decoded = tokenizer.decode(ids)\n",
    "\n",
    "print('token IDs:', ids)\n",
    "print('decoded  :', decoded)\n",
    "assert decoded == sample, 'Roundtrip failed'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
