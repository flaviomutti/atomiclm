{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Understanding Rotary Position Embedding (RoPE)\n",
    "\n",
    "This notebook explains **RoPE (Rotary Position Embedding)** step-by-step, the positional encoding mechanism used in LLaMA, GPT-NeoX, and other modern LLMs.\n",
    "\n",
    "## Why RoPE?\n",
    "\n",
    "In transformers, attention is **permutation invariant** — without positional information, \"dog bites man\" and \"man bites dog\" look identical. We need to encode position.\n",
    "\n",
    "Traditional approaches:\n",
    "- **Absolute positional embeddings** (original Transformer): add learned position vectors to token embeddings\n",
    "- **Sinusoidal embeddings**: fixed sin/cos patterns, good for extrapolation\n",
    "\n",
    "**RoPE** does something smarter: it encodes **relative** position directly into the attention mechanism by rotating query and key vectors in a way that makes their dot product depend on distance.\n",
    "\n",
    "## Key Insight\n",
    "\n",
    "Instead of adding positional info to embeddings, RoPE **rotates** Q and K vectors by angles proportional to their positions. The dot product between rotated vectors naturally captures relative distance.\n",
    "\n",
    "Mathematical property:\n",
    "```\n",
    "Q_m · K_n = (Rotate(Q, m)) · (Rotate(K, n)) = function(m - n)\n",
    "```\n",
    "\n",
    "The attention score depends only on the *difference* between positions `m` and `n`, not their absolute values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d-rotation",
   "metadata": {},
   "source": [
    "## Step 1: 2D Rotation Basics\n",
    "\n",
    "RoPE is built on **2D rotations**. Let's start with the simplest case.\n",
    "\n",
    "A 2D vector `(x, y)` rotated by angle `θ` becomes:\n",
    "```\n",
    "x' = x * cos(θ) - y * sin(θ)\n",
    "y' = x * sin(θ) + y * cos(θ)\n",
    "```\n",
    "\n",
    "In matrix form:\n",
    "```\n",
    "[x']   [cos(θ)  -sin(θ)] [x]\n",
    "[y'] = [sin(θ)   cos(θ)] [y]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "rotate-2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: (1.0, 0.5)\n",
      "After 90° rotation: (-0.49999999999999994, 1.0)\n",
      "After 180° rotation: (-1.0, -0.4999999999999999)\n"
     ]
    }
   ],
   "source": [
    "def rotate_2d(x, y, theta):\n",
    "    \"\"\"Rotate a 2D point (x, y) by angle theta.\"\"\"\n",
    "    x_rot = x * np.cos(theta) - y * np.sin(theta)\n",
    "    y_rot = x * np.sin(theta) + y * np.cos(theta)\n",
    "    return x_rot, y_rot\n",
    "\n",
    "# Example\n",
    "x, y = 1.0, 0.5\n",
    "print(f\"Original: ({x}, {y})\")\n",
    "print(f\"After 90° rotation: {rotate_2d(x, y, np.pi/2)}\")\n",
    "print(f\"After 180° rotation: {rotate_2d(x, y, np.pi)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rope-idea",
   "metadata": {},
   "source": [
    "## Step 2: Extending to Higher Dimensions\n",
    "\n",
    "Attention heads have dimension `d_head` (e.g., 64, 128). RoPE applies 2D rotation to **pairs of dimensions**:\n",
    "\n",
    "- Dimensions 0 and 1 form a pair → rotate by `θ_0 * pos`\n",
    "- Dimensions 2 and 3 form a pair → rotate by `θ_1 * pos`\n",
    "- Dimensions 4 and 5 form a pair → rotate by `θ_2 * pos`\n",
    "- ...\n",
    "\n",
    "Each pair has a **different base frequency** `θ_i`, computed as:\n",
    "```\n",
    "θ_i = 1 / (base^(2i / d))\n",
    "```\n",
    "\n",
    "Default `base = 10000` (like sinusoidal embeddings). Lower frequencies for early dimensions (global patterns), higher frequencies for later dimensions (fine-grained patterns).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "frequencies",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head dimension: 8\n",
      "Number of rotation pairs: 4\n",
      "Frequencies: [1.    0.1   0.01  0.001]\n",
      "\n",
      "Frequency ratio (first/last): 1000.00\n",
      "Lower frequencies (early dims) capture global patterns.\n",
      "Higher frequencies (late dims) capture fine-grained patterns.\n"
     ]
    }
   ],
   "source": [
    "def compute_freqs(d_head, base=10000.0):\n",
    "    \"\"\"\n",
    "    Compute rotation frequencies for RoPE.\n",
    "    \n",
    "    Args:\n",
    "        d_head: Dimension of each attention head (must be even)\n",
    "        base: Base for frequency computation (default 10000)\n",
    "    \n",
    "    Returns:\n",
    "        freqs: Array of shape (d_head // 2,) with rotation frequencies\n",
    "    \"\"\"\n",
    "    # Pair indices: 0, 1, 2, ... d_head//2 - 1\n",
    "    i = np.arange(0, d_head, 2)  # [0, 2, 4, ...]\n",
    "    \n",
    "    # Compute θ_i = 1 / (base^(2i / d))\n",
    "    freqs = 1.0 / (base ** (i / d_head))\n",
    "    \n",
    "    return freqs\n",
    "\n",
    "# Example: 8-dimensional head\n",
    "d_head = 8\n",
    "freqs = compute_freqs(d_head)\n",
    "\n",
    "print(f\"Head dimension: {d_head}\")\n",
    "print(f\"Number of rotation pairs: {len(freqs)}\")\n",
    "print(f\"Frequencies: {freqs}\")\n",
    "print(f\"\\nFrequency ratio (first/last): {freqs[0] / freqs[-1]:.2f}\")\n",
    "print(\"Lower frequencies (early dims) capture global patterns.\")\n",
    "print(\"Higher frequencies (late dims) capture fine-grained patterns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apply-rope",
   "metadata": {},
   "source": [
    "## Step 3: Applying RoPE to a Vector\n",
    "\n",
    "Given:\n",
    "- A vector `x` of dimension `d_head`\n",
    "- Position `pos` in the sequence\n",
    "- Frequencies `θ = [θ_0, θ_1, ..., θ_{d/2-1}]`\n",
    "\n",
    "We split `x` into pairs and rotate each pair:\n",
    "```python\n",
    "for i in range(d_head // 2):\n",
    "    x[2*i], x[2*i+1] = rotate_2d(x[2*i], x[2*i+1], θ_i * pos)\n",
    "```\n",
    "\n",
    "The rotation angle for each pair is `θ_i * pos` — higher positions = more rotation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "apply-rotation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vector:\n",
      "[ 0.49671415 -0.1382643   0.64768854  1.52302986 -0.23415337 -0.23413696\n",
      "  1.57921282  0.76743473]\n",
      "\n",
      "After RoPE at position 0:\n",
      "[ 0.49671415 -0.1382643   0.64768854  1.52302986 -0.23415337 -0.23413696\n",
      "  1.57921282  0.76743473]\n",
      "\n",
      "After RoPE at position 5:\n",
      "[ 0.00831403 -0.51553161 -0.16177924  1.64710287 -0.22215877 -0.24554714\n",
      "  1.57535592  0.77532117]\n",
      "\n",
      "After RoPE at position 100:\n",
      "[ 0.3583137  -0.3707469   0.28510338 -1.63028723  0.07050585 -0.32353801\n",
      "  1.4947077   0.92125896]\n",
      "\n",
      "Notice: Same vector, different positions → different rotations\n"
     ]
    }
   ],
   "source": [
    "def apply_rope_naive(x, pos, freqs):\n",
    "    \"\"\"\n",
    "    Apply RoPE rotation to vector x at position pos.\n",
    "    \n",
    "    Args:\n",
    "        x: Vector of shape (d_head,)\n",
    "        pos: Position in sequence (integer)\n",
    "        freqs: Rotation frequencies, shape (d_head // 2,)\n",
    "    \n",
    "    Returns:\n",
    "        Rotated vector of shape (d_head,)\n",
    "    \"\"\"\n",
    "    d_head = len(x)\n",
    "    x_rotated = x.copy()\n",
    "    \n",
    "    for i in range(d_head // 2):\n",
    "        # Get the pair\n",
    "        x0 = x[2*i]\n",
    "        x1 = x[2*i + 1]\n",
    "        \n",
    "        # Rotation angle for this pair\n",
    "        theta = freqs[i] * pos\n",
    "        \n",
    "        # Apply 2D rotation\n",
    "        cos_theta = np.cos(theta)\n",
    "        sin_theta = np.sin(theta)\n",
    "        \n",
    "        x_rotated[2*i]     = x0 * cos_theta - x1 * sin_theta\n",
    "        x_rotated[2*i + 1] = x0 * sin_theta + x1 * cos_theta\n",
    "    \n",
    "    return x_rotated\n",
    "\n",
    "# Example\n",
    "d_head = 8\n",
    "x = np.random.randn(d_head)\n",
    "freqs = compute_freqs(d_head)\n",
    "\n",
    "print(\"Original vector:\")\n",
    "print(x)\n",
    "print(\"\\nAfter RoPE at position 0:\")\n",
    "print(apply_rope_naive(x, pos=0, freqs=freqs))\n",
    "print(\"\\nAfter RoPE at position 5:\")\n",
    "print(apply_rope_naive(x, pos=5, freqs=freqs))\n",
    "print(\"\\nAfter RoPE at position 100:\")\n",
    "print(apply_rope_naive(x, pos=100, freqs=freqs))\n",
    "print(\"\\nNotice: Same vector, different positions → different rotations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-impl",
   "metadata": {},
   "source": [
    "## Step 4: Efficient Implementation with Complex Numbers\n",
    "\n",
    "The naive loop is slow. Instead, we use **complex number multiplication**.\n",
    "\n",
    "Key insight: 2D rotation is complex multiplication:\n",
    "```\n",
    "(x + iy) * e^(iθ) = (x + iy) * (cos(θ) + i*sin(θ))\n",
    "```\n",
    "\n",
    "Steps:\n",
    "1. Precompute `freqs_cis = e^(i * θ_i * pos)` for all positions\n",
    "2. View vector as complex: `[x0, x1, x2, x3, ...]` → `[x0+i*x1, x2+i*x3, ...]`\n",
    "3. Multiply by `freqs_cis`\n",
    "4. View back as real\n",
    "\n",
    "This is the production implementation used in LLaMA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "precompute-freqs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputed freqs_cis shape: (100, 4)\n",
      "Type: complex128\n",
      "\n",
      "For position 0: [1.+0.j 1.+0.j 1.+0.j 1.+0.j]\n",
      "For position 10: [-0.83907153-0.54402111j  0.54030231+0.84147098j  0.99500417+0.09983342j\n",
      "  0.99995   +0.00999983j]\n",
      "\n",
      "This precomputation happens once and gets reused for all sequences.\n"
     ]
    }
   ],
   "source": [
    "def precompute_freqs_cis(d_head, max_seq_len=2048, base=10000.0):\n",
    "    \"\"\"\n",
    "    Precompute rotation matrices for all positions.\n",
    "    \n",
    "    Args:\n",
    "        d_head: Head dimension (must be even)\n",
    "        max_seq_len: Maximum sequence length\n",
    "        base: Frequency base\n",
    "    \n",
    "    Returns:\n",
    "        Complex tensor of shape (max_seq_len, d_head // 2)\n",
    "        Each entry is e^(i * θ_j * pos)\n",
    "    \"\"\"\n",
    "    # Compute base frequencies\n",
    "    freqs = compute_freqs(d_head, base)\n",
    "    \n",
    "    # Create position range\n",
    "    positions = np.arange(max_seq_len)\n",
    "    \n",
    "    # Outer product: (max_seq_len, 1) x (1, d_head//2) = (max_seq_len, d_head//2)\n",
    "    angles = np.outer(positions, freqs)\n",
    "    \n",
    "    # Convert to complex: e^(i*θ) = cos(θ) + i*sin(θ)\n",
    "    freqs_cis = np.cos(angles) + 1j * np.sin(angles)\n",
    "    \n",
    "    return freqs_cis\n",
    "\n",
    "# Precompute for sequence length 100\n",
    "d_head = 8\n",
    "max_seq_len = 100\n",
    "freqs_cis = precompute_freqs_cis(d_head, max_seq_len)\n",
    "\n",
    "print(f\"Precomputed freqs_cis shape: {freqs_cis.shape}\")\n",
    "print(f\"Type: {freqs_cis.dtype}\")\n",
    "print(f\"\\nFor position 0: {freqs_cis[0]}\")\n",
    "print(f\"For position 10: {freqs_cis[10]}\")\n",
    "print(\"\\nThis precomputation happens once and gets reused for all sequences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "apply-complex",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive implementation:\n",
      "[ 0.68505083  0.21326734 -0.17872323  0.63223269  1.00109309 -1.64833239\n",
      " -1.69978754 -0.63421692]\n",
      "\n",
      "Complex implementation:\n",
      "[ 0.68505083  0.21326734 -0.17872323  0.63223269  1.00109309 -1.64833239\n",
      " -1.69978754 -0.63421692]\n",
      "\n",
      "Difference (should be ~0):\n",
      "0.0\n",
      "\n",
      "✅ Both implementations produce identical results!\n"
     ]
    }
   ],
   "source": [
    "def apply_rope_complex(x, freqs_cis_pos):\n",
    "    \"\"\"\n",
    "    Apply RoPE using complex multiplication (fast version).\n",
    "    \n",
    "    Args:\n",
    "        x: Vector of shape (d_head,)\n",
    "        freqs_cis_pos: Precomputed rotation for this position, shape (d_head // 2,)\n",
    "    \n",
    "    Returns:\n",
    "        Rotated vector of shape (d_head,)\n",
    "    \"\"\"\n",
    "    d_head = len(x)\n",
    "    \n",
    "    # View as complex: [x0, x1, x2, x3] -> [x0+i*x1, x2+i*x3]\n",
    "    x_complex = x.reshape(-1, 2).astype(np.complex128)\n",
    "    x_complex = x_complex[:, 0] + 1j * x_complex[:, 1]\n",
    "    \n",
    "    # Apply rotation via complex multiplication\n",
    "    x_rotated_complex = x_complex * freqs_cis_pos\n",
    "    \n",
    "    # Convert back to real\n",
    "    x_rotated = np.stack([x_rotated_complex.real, x_rotated_complex.imag], axis=-1)\n",
    "    x_rotated = x_rotated.flatten()\n",
    "    \n",
    "    return x_rotated\n",
    "\n",
    "# Verify both implementations match\n",
    "x = np.random.randn(8)\n",
    "freqs = compute_freqs(8)\n",
    "freqs_cis = precompute_freqs_cis(8, 100)\n",
    "\n",
    "pos = 42\n",
    "result_naive = apply_rope_naive(x, pos, freqs)\n",
    "result_complex = apply_rope_complex(x, freqs_cis[pos])\n",
    "\n",
    "print(\"Naive implementation:\")\n",
    "print(result_naive)\n",
    "print(\"\\nComplex implementation:\")\n",
    "print(result_complex)\n",
    "print(\"\\nDifference (should be ~0):\")\n",
    "print(np.abs(result_naive - result_complex).max())\n",
    "print(\"\\n✅ Both implementations produce identical results!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-position",
   "metadata": {},
   "source": [
    "## Step 5: The Magic — Relative Position Encoding\n",
    "\n",
    "Here's why RoPE is special. When we compute attention:\n",
    "```\n",
    "score = Q_m · K_n\n",
    "```\n",
    "\n",
    "With RoPE:\n",
    "```\n",
    "score = Rotate(Q, m) · Rotate(K, n)\n",
    "```\n",
    "\n",
    "Due to rotation properties, this depends only on `m - n`, the **relative distance**.\n",
    "\n",
    "Let's verify this experimentally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "relative-demo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: Same distance, different absolute positions\n",
      "============================================================\n",
      "Positions (  0,   5), distance=5: score = -1.844244\n",
      "Positions ( 10,  15), distance=5: score = -1.844244\n",
      "Positions ( 50,  55), distance=5: score = -1.844244\n",
      "Positions (100, 105), distance=5: score = -1.844244\n",
      "\n",
      "Score variance: 0.0000000000\n",
      "✅ All scores nearly identical — depends only on distance!\n",
      "\n",
      "============================================================\n",
      "Testing: Different distances from same starting position\n",
      "============================================================\n",
      "Distance  0: score = -2.393375\n",
      "Distance  1: score = -2.512100\n",
      "Distance  5: score = -1.844244\n",
      "Distance 10: score = -1.953404\n",
      "Distance 20: score = -1.591033\n",
      "Distance 50: score = -4.243366\n",
      "\n",
      "✅ Scores vary with distance, not absolute position!\n"
     ]
    }
   ],
   "source": [
    "# Create two random vectors\n",
    "q = np.random.randn(8)\n",
    "k = np.random.randn(8)\n",
    "freqs_cis = precompute_freqs_cis(8, 200)\n",
    "\n",
    "# Compute attention scores for different (m, n) pairs with same distance\n",
    "distance = 5\n",
    "pairs = [(0, 5), (10, 15), (50, 55), (100, 105)]  # All have distance = 5\n",
    "\n",
    "print(\"Testing: Same distance, different absolute positions\")\n",
    "print(\"=\"*60)\n",
    "scores = []\n",
    "for m, n in pairs:\n",
    "    q_rot = apply_rope_complex(q, freqs_cis[m])\n",
    "    k_rot = apply_rope_complex(k, freqs_cis[n])\n",
    "    score = np.dot(q_rot, k_rot)\n",
    "    scores.append(score)\n",
    "    print(f\"Positions ({m:3d}, {n:3d}), distance={n-m}: score = {score:.6f}\")\n",
    "\n",
    "print(f\"\\nScore variance: {np.var(scores):.10f}\")\n",
    "print(\"✅ All scores nearly identical — depends only on distance!\")\n",
    "\n",
    "# Now try different distances\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Testing: Different distances from same starting position\")\n",
    "print(\"=\"*60)\n",
    "m = 10\n",
    "distances = [0, 1, 5, 10, 20, 50]\n",
    "for d in distances:\n",
    "    n = m + d\n",
    "    q_rot = apply_rope_complex(q, freqs_cis[m])\n",
    "    k_rot = apply_rope_complex(k, freqs_cis[n])\n",
    "    score = np.dot(q_rot, k_rot)\n",
    "    print(f\"Distance {d:2d}: score = {score:.6f}\")\n",
    "\n",
    "print(\"\\n✅ Scores vary with distance, not absolute position!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pytorch-impl",
   "metadata": {},
   "source": [
    "## Step 6: PyTorch Implementation\n",
    "\n",
    "Now let's implement RoPE in PyTorch, matching the production code style.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "torch-rope",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoPE module initialized: head_dim=64, max_seq_len=512\n",
      "Precomputed freqs_cis shape: torch.Size([512, 32])\n",
      "\n",
      "Frequencies for seq_len=16: torch.Size([16, 32])\n",
      "\n",
      "Original Q shape: torch.Size([2, 8, 16, 64])\n",
      "Rotated Q shape: torch.Size([2, 8, 16, 64])\n",
      "Shape preserved: True\n"
     ]
    }
   ],
   "source": [
    "class RotaryEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Rotary Position Embedding (RoPE).\n",
    "    \n",
    "    Precomputes rotation frequencies and applies them to Q/K tensors.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, head_dim: int, max_seq_len: int = 2048, base: float = 10000.0):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.base = base\n",
    "        \n",
    "        # Precompute frequencies\n",
    "        freqs_cis = self._precompute_freqs_cis()\n",
    "        # Register as buffer (not a parameter, moves with model to device)\n",
    "        self.register_buffer('freqs_cis', freqs_cis, persistent=False)\n",
    "    \n",
    "    def _precompute_freqs_cis(self):\n",
    "        # Compute base frequencies: θ_i = 1 / (base^(2i / d))\n",
    "        i = torch.arange(0, self.head_dim, 2, dtype=torch.float32)\n",
    "        freqs = 1.0 / (self.base ** (i / self.head_dim))\n",
    "        \n",
    "        # Positions\n",
    "        positions = torch.arange(self.max_seq_len, dtype=torch.float32)\n",
    "        \n",
    "        # Outer product: (max_seq_len, head_dim // 2)\n",
    "        angles = torch.outer(positions, freqs)\n",
    "        \n",
    "        # Convert to complex: e^(iθ) = cos(θ) + i*sin(θ)\n",
    "        freqs_cis = torch.polar(torch.ones_like(angles), angles)\n",
    "        \n",
    "        return freqs_cis\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_rotary(x: torch.Tensor, freqs_cis: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply rotary embedding to input tensor.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch, heads, seq_len, head_dim)\n",
    "            freqs_cis: Rotation frequencies of shape (seq_len, head_dim // 2)\n",
    "        \n",
    "        Returns:\n",
    "            Rotated tensor of same shape as x\n",
    "        \"\"\"\n",
    "        # Reshape x to pair up dimensions: (b, h, t, d) -> (b, h, t, d//2, 2)\n",
    "        x_reshaped = x.float().reshape(*x.shape[:-1], -1, 2)\n",
    "        \n",
    "        # Convert to complex: (b, h, t, d//2)\n",
    "        x_complex = torch.view_as_complex(x_reshaped)\n",
    "        \n",
    "        # Broadcast freqs_cis: (t, d//2) -> (1, 1, t, d//2)\n",
    "        freqs_cis = freqs_cis.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        # Apply rotation via complex multiplication\n",
    "        x_rotated = x_complex * freqs_cis\n",
    "        \n",
    "        # Convert back to real: (b, h, t, d//2, 2) -> (b, h, t, d)\n",
    "        x_out = torch.view_as_real(x_rotated).flatten(-2)\n",
    "        \n",
    "        return x_out.type_as(x)\n",
    "    \n",
    "    def forward(self, seq_len: int, offset: int = 0) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Get rotation frequencies for a given sequence.\n",
    "        \n",
    "        Args:\n",
    "            seq_len: Length of current sequence\n",
    "            offset: Starting position (for KV cache)\n",
    "        \n",
    "        Returns:\n",
    "            freqs_cis slice of shape (seq_len, head_dim // 2)\n",
    "        \"\"\"\n",
    "        return self.freqs_cis[offset : offset + seq_len]\n",
    "\n",
    "# Test the implementation\n",
    "rope = RotaryEmbedding(head_dim=64, max_seq_len=512)\n",
    "print(f\"RoPE module initialized: head_dim={rope.head_dim}, max_seq_len={rope.max_seq_len}\")\n",
    "print(f\"Precomputed freqs_cis shape: {rope.freqs_cis.shape}\")\n",
    "\n",
    "# Create dummy Q/K tensors: (batch, heads, seq_len, head_dim)\n",
    "batch_size = 2\n",
    "num_heads = 8\n",
    "seq_len = 16\n",
    "head_dim = 64\n",
    "\n",
    "q = torch.randn(batch_size, num_heads, seq_len, head_dim)\n",
    "k = torch.randn(batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "# Get frequencies for this sequence\n",
    "freqs_cis = rope(seq_len)\n",
    "print(f\"\\nFrequencies for seq_len={seq_len}: {freqs_cis.shape}\")\n",
    "\n",
    "# Apply RoPE\n",
    "q_rotated = rope.apply_rotary(q, freqs_cis)\n",
    "k_rotated = rope.apply_rotary(k, freqs_cis)\n",
    "\n",
    "print(f\"\\nOriginal Q shape: {q.shape}\")\n",
    "print(f\"Rotated Q shape: {q_rotated.shape}\")\n",
    "print(f\"Shape preserved: {q.shape == q_rotated.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usage-attention",
   "metadata": {},
   "source": [
    "## Step 7: Using RoPE in Attention\n",
    "\n",
    "In practice, RoPE is applied in the attention layer:\n",
    "\n",
    "```python\n",
    "# Inside MultiHeadAttention.forward()\n",
    "qkv = self.qkv_proj(x)  # (b, t, 3*d_out)\n",
    "q, k, v = split_and_reshape(qkv)  # (b, h, t, d_h)\n",
    "\n",
    "# Apply RoPE BEFORE KV-caching\n",
    "if freqs_cis is not None:\n",
    "    q = RotaryEmbedding.apply_rotary(q, freqs_cis)\n",
    "    k = RotaryEmbedding.apply_rotary(k, freqs_cis)\n",
    "\n",
    "# Then compute attention as usual\n",
    "scores = q @ k.transpose(-2, -1) / sqrt(d_h)\n",
    "attn = softmax(scores, dim=-1)\n",
    "output = attn @ v\n",
    "```\n",
    "\n",
    "**Key**: RoPE is applied to Q and K but **NOT V** — only positional information goes into the attention weights, not the values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "attention-demo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 4, 10, 32])\n",
      "Attention weights shape: torch.Size([1, 4, 10, 10])\n",
      "\n",
      "Attention matrix for first head (first 5x5):\n",
      "[[0.03593255 0.08071355 0.23664361 0.05539827 0.04589043]\n",
      " [0.27033275 0.15126061 0.06344225 0.02910307 0.06042084]\n",
      " [0.02558617 0.06199391 0.15081383 0.08664306 0.08330887]\n",
      " [0.19717795 0.14761339 0.02668178 0.0051381  0.01747172]\n",
      " [0.17322601 0.11085868 0.04897138 0.1102547  0.03070857]]\n"
     ]
    }
   ],
   "source": [
    "def simple_attention_with_rope(q, k, v, rope_module, seq_len):\n",
    "    \"\"\"\n",
    "    Simplified attention with RoPE.\n",
    "    \n",
    "    Args:\n",
    "        q, k, v: Tensors of shape (batch, heads, seq_len, head_dim)\n",
    "        rope_module: RotaryEmbedding instance\n",
    "        seq_len: Sequence length\n",
    "    \n",
    "    Returns:\n",
    "        Attention output of same shape as v\n",
    "    \"\"\"\n",
    "    # Get RoPE frequencies\n",
    "    freqs_cis = rope_module(seq_len)\n",
    "    \n",
    "    # Apply RoPE to Q and K (NOT V)\n",
    "    q = rope_module.apply_rotary(q, freqs_cis)\n",
    "    k = rope_module.apply_rotary(k, freqs_cis)\n",
    "    \n",
    "    # Scaled dot-product attention\n",
    "    head_dim = q.size(-1)\n",
    "    scores = (q @ k.transpose(-2, -1)) / (head_dim ** 0.5)\n",
    "    attn = torch.softmax(scores, dim=-1)\n",
    "    output = attn @ v\n",
    "    \n",
    "    return output, attn\n",
    "\n",
    "# Demo\n",
    "rope = RotaryEmbedding(head_dim=32)\n",
    "seq_len = 10\n",
    "\n",
    "q = torch.randn(1, 4, seq_len, 32)\n",
    "k = torch.randn(1, 4, seq_len, 32)\n",
    "v = torch.randn(1, 4, seq_len, 32)\n",
    "\n",
    "output, attn = simple_attention_with_rope(q, k, v, rope, seq_len)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn.shape}\")\n",
    "print(f\"\\nAttention matrix for first head (first 5x5):\")\n",
    "print(attn[0, 0, :5, :5].detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**RoPE (Rotary Position Embedding)** encodes position by rotating Q and K vectors:\n",
    "\n",
    "1. **Split dimensions into pairs** — each pair gets rotated independently\n",
    "2. **Different frequencies** — early dims use low freq (global), late dims use high freq (local)\n",
    "3. **Position-dependent rotation** — angle = `θ_i * position`\n",
    "4. **Relative encoding** — attention score `Q_m · K_n` depends only on `m - n`\n",
    "5. **Efficient via complex numbers** — precompute `e^(iθ)`, multiply as complex\n",
    "\n",
    "### Advantages over absolute embeddings:\n",
    "- ✅ Naturally encodes **relative** position (\"how far apart are tokens?\")\n",
    "- ✅ Better **extrapolation** to longer sequences than seen in training\n",
    "- ✅ No learned parameters — purely geometric\n",
    "- ✅ Works seamlessly with **KV-cache** (position baked into cached keys)\n",
    "\n",
    "### Where RoPE is used:\n",
    "- LLaMA (all versions)\n",
    "- GPT-NeoX\n",
    "- PaLM\n",
    "- Many modern open-source LLMs\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Integrate `RotaryEmbedding` into your `MultiHeadAttention` layer\n",
    "2. Precompute `freqs_cis` once in the model constructor\n",
    "3. Pass `freqs_cis` slice to each attention layer during forward pass\n",
    "4. Train and see if RoPE improves long-range dependencies!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
