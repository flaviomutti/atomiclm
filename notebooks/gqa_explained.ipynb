{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Grouped Query Attention (GQA): Memory-Efficient Inference\n",
    "\n",
    "This notebook explains **Grouped Query Attention (GQA)**, the memory optimization technique used in Llama 2/3, Mistral, and other modern LLMs to reduce KV-cache size during inference.\n",
    "\n",
    "## The Problem: KV-Cache Memory Explosion\n",
    "\n",
    "In autoregressive decoding, we cache keys and values from previous tokens to avoid recomputing them:\n",
    "\n",
    "```python\n",
    "# Without cache: recompute everything at each step O(T²)\n",
    "for t in range(max_len):\n",
    "    logits = model(tokens[:t+1])  # Processes all previous tokens\n",
    "\n",
    "# With cache: reuse previous K/V O(T)\n",
    "cache = model.make_cache()\n",
    "for t in range(max_len):\n",
    "    logits, cache = model(tokens[t:t+1], kv_cache=cache)  # Only new token\n",
    "```\n",
    "\n",
    "**Cache memory per layer:**\n",
    "```\n",
    "K cache: (batch, num_heads, seq_len, head_dim)\n",
    "V cache: (batch, num_heads, seq_len, head_dim)\n",
    "```\n",
    "\n",
    "**Example:** 8 heads, seq_len=2048, head_dim=64, fp32\n",
    "- K cache: 1 × 8 × 2048 × 64 × 4 bytes = **4 MB**\n",
    "- V cache: 1 × 8 × 2048 × 64 × 4 bytes = **4 MB**\n",
    "- **Total per layer: 8 MB**\n",
    "- **12 layers: 96 MB** (just for one sequence!)\n",
    "\n",
    "This scales linearly with context length. At 32k context: **1.5 GB per sequence**.\n",
    "\n",
    "## The Insight: Do Queries and Keys Need the Same Number of Heads?\n",
    "\n",
    "Standard Multi-Head Attention (MHA):\n",
    "- 8 query heads → 8 key heads → 8 value heads\n",
    "- Each query head attends to its own dedicated K/V head\n",
    "\n",
    "**GQA asks:** What if multiple query heads *share* the same K/V heads?\n",
    "\n",
    "**Grouped Query Attention:**\n",
    "- 8 query heads → **2 key heads** → 2 value heads\n",
    "- 4 query heads per group, each group shares one K/V head\n",
    "- **Cache memory: 4× smaller** (2 KV heads instead of 8)\n",
    "\n",
    "**Multi-Query Attention (MQA):** Extreme case with num_kv_heads=1\n",
    "- 8 query heads → **1 key head** → 1 value head\n",
    "- All queries share a single K/V head\n",
    "- **Cache memory: 8× smaller**\n",
    "- Used in PaLM, Falcon (but can hurt quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10fb51870>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Import our GQA-enabled attention\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from atomiclm.model.attention import MultiHeadAttention\n",
    "from atomiclm.model.decoder import Decoder\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variants",
   "metadata": {},
   "source": [
    "## Three Attention Variants, One Implementation\n",
    "\n",
    "Our `MultiHeadAttention` supports all three via a single `num_kv_heads` parameter:\n",
    "\n",
    "| Variant | `num_kv_heads` | Example (8 heads) | Cache Size | Quality |\n",
    "|---------|----------------|-------------------|------------|---------|\n",
    "| **MHA** | `num_heads` | 8 Q, 8 KV | 100% | Best |\n",
    "| **GQA** | `1 < x < num_heads` | 8 Q, 2 KV | 25% | ~MHA |\n",
    "| **MQA** | `1` | 8 Q, 1 KV | 12.5% | Good |\n",
    "\n",
    "**Key constraint:** `num_heads` must be divisible by `num_kv_heads` (queries group evenly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "create-variants",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MHA:\n",
      "  num_heads=8, num_kv_heads=8, num_groups=1\n",
      "\n",
      "GQA (2 KV heads):\n",
      "  num_heads=8, num_kv_heads=2, num_groups=4\n",
      "\n",
      "GQA (4 KV heads):\n",
      "  num_heads=8, num_kv_heads=4, num_groups=2\n",
      "\n",
      "MQA:\n",
      "  num_heads=8, num_kv_heads=1, num_groups=8\n",
      "\n",
      "num_groups = queries per KV head\n"
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "num_heads = 8\n",
    "head_dim = d_model // num_heads  # 64\n",
    "\n",
    "# Multi-Head Attention (standard)\n",
    "mha = MultiHeadAttention(\n",
    "    d_in=d_model,\n",
    "    d_out=d_model,\n",
    "    num_heads=num_heads,\n",
    "    num_kv_heads=num_heads,  # Same as num_heads\n",
    ")\n",
    "\n",
    "# Grouped Query Attention (2 groups)\n",
    "gqa_2 = MultiHeadAttention(\n",
    "    d_in=d_model,\n",
    "    d_out=d_model,\n",
    "    num_heads=num_heads,\n",
    "    num_kv_heads=2,  # 8 queries share 2 KV heads\n",
    ")\n",
    "\n",
    "# Grouped Query Attention (4 groups)\n",
    "gqa_4 = MultiHeadAttention(\n",
    "    d_in=d_model,\n",
    "    d_out=d_model,\n",
    "    num_heads=num_heads,\n",
    "    num_kv_heads=4,  # 8 queries share 4 KV heads\n",
    ")\n",
    "\n",
    "# Multi-Query Attention (single KV head)\n",
    "mqa = MultiHeadAttention(\n",
    "    d_in=d_model,\n",
    "    d_out=d_model,\n",
    "    num_heads=num_heads,\n",
    "    num_kv_heads=1,  # All 8 queries share 1 KV head\n",
    ")\n",
    "\n",
    "print(\"MHA:\")\n",
    "print(f\"  num_heads={mha.num_heads}, num_kv_heads={mha.num_kv_heads}, num_groups={mha.num_groups}\")\n",
    "print(\"\\nGQA (2 KV heads):\")\n",
    "print(f\"  num_heads={gqa_2.num_heads}, num_kv_heads={gqa_2.num_kv_heads}, num_groups={gqa_2.num_groups}\")\n",
    "print(\"\\nGQA (4 KV heads):\")\n",
    "print(f\"  num_heads={gqa_4.num_heads}, num_kv_heads={gqa_4.num_kv_heads}, num_groups={gqa_4.num_groups}\")\n",
    "print(\"\\nMQA:\")\n",
    "print(f\"  num_heads={mqa.num_heads}, num_kv_heads={mqa.num_kv_heads}, num_groups={mqa.num_groups}\")\n",
    "print(\"\\nnum_groups = queries per KV head\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "implementation",
   "metadata": {},
   "source": [
    "## How GQA Works: Implementation Details\n",
    "\n",
    "### 1. Separate Q/K/V Projections\n",
    "\n",
    "Standard MHA uses a fused QKV projection:\n",
    "```python\n",
    "# MHA: single projection\n",
    "qkv = self.qkv_proj(x)  # (b, t, 3 * d_out)\n",
    "q, k, v = split(qkv)    # Each: (b, num_heads, t, head_dim)\n",
    "```\n",
    "\n",
    "GQA uses separate projections with different output sizes:\n",
    "```python\n",
    "# GQA: separate projections\n",
    "q = self.q_proj(x)  # (b, t, num_heads * head_dim)\n",
    "k = self.k_proj(x)  # (b, t, num_kv_heads * head_dim)  ← smaller!\n",
    "v = self.v_proj(x)  # (b, t, num_kv_heads * head_dim)  ← smaller!\n",
    "```\n",
    "\n",
    "### 2. KV Expansion Before Attention\n",
    "\n",
    "We store K/V in compressed form but expand before attention:\n",
    "```python\n",
    "# After projection: k, v are (b, num_kv_heads, t, head_dim)\n",
    "\n",
    "# Expand each KV head to match num_groups query heads\n",
    "k = repeat_kv(k, num_groups)  # (b, num_heads, t, head_dim)\n",
    "v = repeat_kv(v, num_groups)  # (b, num_heads, t, head_dim)\n",
    "\n",
    "# Now attention is identical to MHA\n",
    "scores = q @ k.transpose(-2, -1)\n",
    "attn = softmax(scores, dim=-1)\n",
    "output = attn @ v\n",
    "```\n",
    "\n",
    "### 3. The `repeat_kv` Helper\n",
    "\n",
    "```python\n",
    "@staticmethod\n",
    "def _repeat_kv(x: Tensor, num_groups: int) -> Tensor:\n",
    "    \"\"\"Expand (b, num_kv_heads, t, d) to (b, num_heads, t, d)\"\"\"\n",
    "    if num_groups == 1:  # MHA case, no expansion needed\n",
    "        return x\n",
    "    \n",
    "    b, num_kv_heads, t, d_h = x.shape\n",
    "    # Unsqueeze: (b, num_kv_heads, 1, t, d_h)\n",
    "    # Expand:   (b, num_kv_heads, num_groups, t, d_h)\n",
    "    x = x.unsqueeze(2).expand(b, num_kv_heads, num_groups, t, d_h)\n",
    "    # Reshape:  (b, num_kv_heads * num_groups, t, d_h)\n",
    "    return x.reshape(b, num_kv_heads * num_groups, t, d_h)\n",
    "```\n",
    "\n",
    "**Example:** 8 query heads, 2 KV heads\n",
    "- Input K: `(1, 2, 100, 64)` — 2 KV heads\n",
    "- After expansion: `(1, 8, 100, 64)` — 8 heads (each KV head repeated 4 times)\n",
    "- KV heads 0-3 all use the same K[0], KV heads 4-7 use K[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "repeat-kv-demo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original KV (2 heads):\n",
      "Shape: torch.Size([1, 2, 4, 8])\n",
      "Head 0, position 0: tensor([0., 1., 2., 3.])\n",
      "Head 1, position 0: tensor([32., 33., 34., 35.])\n",
      "\n",
      "Expanded KV (8 heads):\n",
      "Shape: torch.Size([1, 8, 4, 8])\n",
      "\n",
      "Heads 0-3 should match original head 0:\n",
      "  Head 0, pos 0: tensor([0., 1., 2., 3.])\n",
      "  Head 1, pos 0: tensor([0., 1., 2., 3.])\n",
      "  Head 2, pos 0: tensor([0., 1., 2., 3.])\n",
      "  Head 3, pos 0: tensor([0., 1., 2., 3.])\n",
      "\n",
      "Heads 4-7 should match original head 1:\n",
      "  Head 4, pos 0: tensor([32., 33., 34., 35.])\n",
      "  Head 5, pos 0: tensor([32., 33., 34., 35.])\n",
      "  Head 6, pos 0: tensor([32., 33., 34., 35.])\n",
      "  Head 7, pos 0: tensor([32., 33., 34., 35.])\n",
      "\n",
      "✅ Each original KV head is repeated num_groups times\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate repeat_kv\n",
    "batch = 1\n",
    "num_kv_heads = 2\n",
    "seq_len = 4\n",
    "head_dim = 8\n",
    "\n",
    "# Create dummy KV tensor with distinct values per head\n",
    "kv = torch.arange(batch * num_kv_heads * seq_len * head_dim, dtype=torch.float32)\n",
    "kv = kv.reshape(batch, num_kv_heads, seq_len, head_dim)\n",
    "\n",
    "print(\"Original KV (2 heads):\")\n",
    "print(f\"Shape: {kv.shape}\")\n",
    "print(f\"Head 0, position 0: {kv[0, 0, 0, :4]}\")\n",
    "print(f\"Head 1, position 0: {kv[0, 1, 0, :4]}\")\n",
    "\n",
    "# Expand by num_groups=4 (to get 8 total heads)\n",
    "num_groups = 4\n",
    "kv_expanded = MultiHeadAttention._repeat_kv(kv, num_groups)\n",
    "\n",
    "print(f\"\\nExpanded KV (8 heads):\")\n",
    "print(f\"Shape: {kv_expanded.shape}\")\n",
    "print(f\"\\nHeads 0-3 should match original head 0:\")\n",
    "for h in range(4):\n",
    "    print(f\"  Head {h}, pos 0: {kv_expanded[0, h, 0, :4]}\")\n",
    "print(f\"\\nHeads 4-7 should match original head 1:\")\n",
    "for h in range(4, 8):\n",
    "    print(f\"  Head {h}, pos 0: {kv_expanded[0, h, 0, :4]}\")\n",
    "\n",
    "print(\"\\n✅ Each original KV head is repeated num_groups times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "memory-analysis",
   "metadata": {},
   "source": [
    "## Memory Comparison: MHA vs GQA vs MQA\n",
    "\n",
    "Let's compute actual memory usage for different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "memory-calc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: 8 heads, 64-dim, 12 layers, seq_len=2048\n",
      "\n",
      "Variant    KV Heads   Memory (MB)     vs MHA    \n",
      "==================================================\n",
      "MHA        8          96.00           baseline\n",
      "GQA-4      4          48.00           2.0× smaller\n",
      "GQA-2      2          24.00           4.0× smaller\n",
      "MQA        1          12.00           8.0× smaller\n",
      "\n",
      "✅ GQA-2 uses 4× less memory than MHA\n",
      "✅ MQA uses 8× less memory than MHA\n"
     ]
    }
   ],
   "source": [
    "def cache_memory_mb(batch_size, num_kv_heads, seq_len, head_dim, num_layers, dtype=torch.float32):\n",
    "    \"\"\"\n",
    "    Calculate KV-cache memory in MB.\n",
    "    \n",
    "    Cache shape per layer: 2 × (batch, num_kv_heads, seq_len, head_dim)\n",
    "    \"\"\"\n",
    "    bytes_per_element = 4 if dtype == torch.float32 else 2  # fp32 or fp16\n",
    "    elements_per_layer = 2 * batch_size * num_kv_heads * seq_len * head_dim\n",
    "    bytes_per_layer = elements_per_layer * bytes_per_element\n",
    "    total_bytes = bytes_per_layer * num_layers\n",
    "    return total_bytes / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "# Configuration\n",
    "batch_size = 1\n",
    "num_heads = 8\n",
    "head_dim = 64\n",
    "num_layers = 12\n",
    "seq_len = 2048\n",
    "\n",
    "configs = [\n",
    "    (\"MHA\", num_heads),\n",
    "    (\"GQA-4\", 4),\n",
    "    (\"GQA-2\", 2),\n",
    "    (\"MQA\", 1),\n",
    "]\n",
    "\n",
    "print(f\"Configuration: {num_heads} heads, {head_dim}-dim, {num_layers} layers, seq_len={seq_len}\\n\")\n",
    "print(f\"{'Variant':<10} {'KV Heads':<10} {'Memory (MB)':<15} {'vs MHA':<10}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "mha_memory = None\n",
    "for name, num_kv_heads in configs:\n",
    "    memory = cache_memory_mb(batch_size, num_kv_heads, seq_len, head_dim, num_layers)\n",
    "    if mha_memory is None:\n",
    "        mha_memory = memory\n",
    "        ratio = \"baseline\"\n",
    "    else:\n",
    "        ratio = f\"{mha_memory / memory:.1f}× smaller\"\n",
    "    print(f\"{name:<10} {num_kv_heads:<10} {memory:<15.2f} {ratio}\")\n",
    "\n",
    "print(f\"\\n✅ GQA-2 uses 4× less memory than MHA\")\n",
    "print(f\"✅ MQA uses 8× less memory than MHA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "memory-plot",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context    MHA (MB)     GQA-2 (MB)   MQA (MB)    \n",
      "==============================================\n",
      "512        24.0         6.0          3.0         \n",
      "1024       48.0         12.0         6.0         \n",
      "2048       96.0         24.0         12.0        \n",
      "4096       192.0        48.0         24.0        \n",
      "8192       384.0        96.0         48.0        \n",
      "16384      768.0        192.0        96.0        \n",
      "32768      1536.0       384.0        192.0       \n",
      "\n",
      "At 32k context:\n",
      "  MHA: 1536.0 MB\n",
      "  GQA-2: 384.0 MB (4x reduction)\n",
      "  MQA: 192.0 MB (8x reduction)\n"
     ]
    }
   ],
   "source": [
    "# Memory scaling with context length\n",
    "seq_lengths = [512, 1024, 2048, 4096, 8192, 16384, 32768]\n",
    "\n",
    "mha_mem = [cache_memory_mb(1, 8, s, 64, 12) for s in seq_lengths]\n",
    "gqa_2_mem = [cache_memory_mb(1, 2, s, 64, 12) for s in seq_lengths]\n",
    "mqa_mem = [cache_memory_mb(1, 1, s, 64, 12) for s in seq_lengths]\n",
    "\n",
    "print(f\"{'Context':<10} {'MHA (MB)':<12} {'GQA-2 (MB)':<12} {'MQA (MB)':<12}\")\n",
    "print(\"=\" * 46)\n",
    "for s, m, g, q in zip(seq_lengths, mha_mem, gqa_2_mem, mqa_mem):\n",
    "    print(f\"{s:<10} {m:<12.1f} {g:<12.1f} {q:<12.1f}\")\n",
    "\n",
    "print(f\"\\nAt 32k context:\")\n",
    "print(f\"  MHA: {mha_mem[-1]:.1f} MB\")\n",
    "print(f\"  GQA-2: {gqa_2_mem[-1]:.1f} MB (4x reduction)\")\n",
    "print(f\"  MQA: {mqa_mem[-1]:.1f} MB (8x reduction)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correctness",
   "metadata": {},
   "source": [
    "## Correctness: GQA Cached Generation\n",
    "\n",
    "Verify that GQA with KV-cache produces identical output to full forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "test-correctness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full forward output shape: torch.Size([1, 16, 128])\n",
      "Cached output shape: torch.Size([1, 16, 128])\n",
      "\n",
      "Max difference: 2.98e-07\n",
      "Outputs match: True\n",
      "\n",
      "✅ GQA with KV-cache produces identical output to full forward\n"
     ]
    }
   ],
   "source": [
    "# Test GQA cached vs full forward\n",
    "torch.manual_seed(0)\n",
    "\n",
    "d_in = 128\n",
    "d_out = 128\n",
    "num_heads = 8\n",
    "num_kv_heads = 2\n",
    "seq_len = 16\n",
    "\n",
    "gqa = MultiHeadAttention(\n",
    "    d_in=d_in,\n",
    "    d_out=d_out,\n",
    "    num_heads=num_heads,\n",
    "    num_kv_heads=num_kv_heads,\n",
    "    dropout=0.0,\n",
    ")\n",
    "gqa.eval()\n",
    "\n",
    "x = torch.randn(1, seq_len, d_in)\n",
    "\n",
    "# Full sequence forward\n",
    "with torch.no_grad():\n",
    "    full_out, _ = gqa(x)\n",
    "\n",
    "# Token-by-token with cache\n",
    "cache = {\n",
    "    \"k\": torch.zeros(1, num_kv_heads, seq_len, d_out // num_heads),\n",
    "    \"v\": torch.zeros(1, num_kv_heads, seq_len, d_out // num_heads),\n",
    "    \"pos\": 0,\n",
    "}\n",
    "\n",
    "cached_outputs = []\n",
    "with torch.no_grad():\n",
    "    for i in range(seq_len):\n",
    "        token = x[:, i:i+1, :]\n",
    "        out, cache = gqa(token, kv_cache=cache)\n",
    "        cached_outputs.append(out)\n",
    "\n",
    "cached_out = torch.cat(cached_outputs, dim=1)\n",
    "\n",
    "# Compare\n",
    "max_diff = (full_out - cached_out).abs().max().item()\n",
    "print(f\"Full forward output shape: {full_out.shape}\")\n",
    "print(f\"Cached output shape: {cached_out.shape}\")\n",
    "print(f\"\\nMax difference: {max_diff:.2e}\")\n",
    "print(f\"Outputs match: {torch.allclose(full_out, cached_out, atol=1e-6)}\")\n",
    "print(\"\\n✅ GQA with KV-cache produces identical output to full forward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decoder-demo",
   "metadata": {},
   "source": [
    "## Using GQA in the Full Decoder\n",
    "\n",
    "Let's create a small decoder model with GQA and verify memory savings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "decoder-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder configuration:\n",
      "  Layers: 6\n",
      "  Heads: 8\n",
      "  d_model: 256\n",
      "  Max sequence length: 1024\n",
      "\n",
      "KV-Cache Memory:\n",
      "  MHA (8 KV heads): 12.00 MB\n",
      "  GQA (2 KV heads): 3.00 MB\n",
      "  Reduction: 4.0×\n",
      "\n",
      "✅ GQA decoder uses 4.0× less cache memory\n"
     ]
    }
   ],
   "source": [
    "# Create two decoders: MHA and GQA\n",
    "vocab_size = 1024\n",
    "d_model = 256\n",
    "num_layers = 6\n",
    "num_heads = 8\n",
    "d_ff = 1024\n",
    "max_seq_len = 1024\n",
    "\n",
    "torch.manual_seed(42)\n",
    "decoder_mha = Decoder(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    num_layers=num_layers,\n",
    "    num_heads=num_heads,\n",
    "    num_kv_heads=num_heads,  # MHA\n",
    "    d_ff=d_ff,\n",
    "    max_seq_len=max_seq_len,\n",
    ")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "decoder_gqa = Decoder(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    num_layers=num_layers,\n",
    "    num_heads=num_heads,\n",
    "    num_kv_heads=2,  # GQA with 2 KV heads\n",
    "    d_ff=d_ff,\n",
    "    max_seq_len=max_seq_len,\n",
    ")\n",
    "\n",
    "# Create caches\n",
    "cache_mha = decoder_mha.make_cache(batch_size=1)\n",
    "cache_gqa = decoder_gqa.make_cache(batch_size=1)\n",
    "\n",
    "# Calculate memory\n",
    "def cache_size_mb(cache):\n",
    "    total_elements = 0\n",
    "    for layer_cache in cache:\n",
    "        total_elements += layer_cache[\"k\"].numel() + layer_cache[\"v\"].numel()\n",
    "    return total_elements * 4 / (1024 ** 2)  # fp32, convert to MB\n",
    "\n",
    "mha_size = cache_size_mb(cache_mha)\n",
    "gqa_size = cache_size_mb(cache_gqa)\n",
    "\n",
    "print(f\"Decoder configuration:\")\n",
    "print(f\"  Layers: {num_layers}\")\n",
    "print(f\"  Heads: {num_heads}\")\n",
    "print(f\"  d_model: {d_model}\")\n",
    "print(f\"  Max sequence length: {max_seq_len}\")\n",
    "print(f\"\\nKV-Cache Memory:\")\n",
    "print(f\"  MHA ({num_heads} KV heads): {mha_size:.2f} MB\")\n",
    "print(f\"  GQA (2 KV heads): {gqa_size:.2f} MB\")\n",
    "print(f\"  Reduction: {mha_size / gqa_size:.1f}×\")\n",
    "print(f\"\\n✅ GQA decoder uses {mha_size / gqa_size:.1f}× less cache memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "generation-demo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt shape: torch.Size([1, 10])\n",
      "Prompt tokens: [6, 467, 955, 710, 679, 907, 439, 81, 584, 620]\n",
      "\n",
      "Generated sequence shape: torch.Size([1, 30])\n",
      "Generated tokens: [6, 467, 955, 710, 679, 907, 439, 81, 584, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620]\n",
      "\n",
      "✅ GQA decoder successfully generates sequences\n"
     ]
    }
   ],
   "source": [
    "# Test generation with GQA decoder\n",
    "decoder_gqa.eval()\n",
    "\n",
    "# Create a prompt\n",
    "prompt = torch.randint(0, vocab_size, (1, 10))\n",
    "print(f\"Prompt shape: {prompt.shape}\")\n",
    "print(f\"Prompt tokens: {prompt[0].tolist()[:10]}\")\n",
    "\n",
    "# Generate\n",
    "torch.manual_seed(0)\n",
    "with torch.no_grad():\n",
    "    output = decoder_gqa.generate(prompt, max_new_tokens=20, temperature=1.0)\n",
    "\n",
    "print(f\"\\nGenerated sequence shape: {output.shape}\")\n",
    "print(f\"Generated tokens: {output[0].tolist()}\")\n",
    "print(f\"\\n✅ GQA decoder successfully generates sequences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "production",
   "metadata": {},
   "source": [
    "## Production Usage: Real Models\n",
    "\n",
    "### Models Using GQA:\n",
    "\n",
    "**Llama 2 (7B):**\n",
    "- 32 query heads, 32 KV heads = **MHA** (baseline)\n",
    "\n",
    "**Llama 2 (70B):**\n",
    "- 64 query heads, **8 KV heads** = GQA with 8 groups\n",
    "- 8× cache reduction vs MHA\n",
    "\n",
    "**Llama 3 (8B, 70B):**\n",
    "- Uses GQA across all model sizes\n",
    "- 8B: 32 query heads, 8 KV heads (4 groups)\n",
    "\n",
    "**Mistral 7B:**\n",
    "- 32 query heads, **8 KV heads** (4 groups)\n",
    "\n",
    "**Qwen 2:**\n",
    "- All sizes use GQA\n",
    "\n",
    "### When to Use Each Variant:\n",
    "\n",
    "| Variant | Use Case |\n",
    "|---------|----------|\n",
    "| **MHA** | Small models (<1B), research, when memory isn't constrained |\n",
    "| **GQA** | **Production default** — best quality/memory trade-off |\n",
    "| **MQA** | Extreme memory constraints, edge deployment, small quality loss acceptable |\n",
    "\n",
    "### Choosing `num_kv_heads`:\n",
    "\n",
    "Common choices:\n",
    "- `num_kv_heads = num_heads` → MHA (baseline quality)\n",
    "- `num_kv_heads = num_heads // 2` → 2× memory reduction\n",
    "- `num_kv_heads = num_heads // 4` → 4× memory reduction (Llama 2/3, Mistral)\n",
    "- `num_kv_heads = num_heads // 8` → 8× memory reduction (Llama 2 70B)\n",
    "- `num_kv_heads = 1` → MQA (max reduction)\n",
    "\n",
    "**Rule of thumb:** More KV heads = better quality, more memory. Start with `num_heads // 4` for good balance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Grouped Query Attention (GQA)** reduces KV-cache memory by sharing K/V heads across multiple query heads:\n",
    "\n",
    "### Key Concepts:\n",
    "1. **Separate Q/K/V projections** — K and V use fewer heads than Q\n",
    "2. **KV expansion** — Repeat each KV head `num_groups` times before attention\n",
    "3. **Unified interface** — Supports MHA, GQA, and MQA via `num_kv_heads` parameter\n",
    "4. **Cache storage** — Store K/V in compressed form, expand only during computation\n",
    "\n",
    "### Memory Savings:\n",
    "- **4× reduction** typical (8 query heads → 2 KV heads)\n",
    "- **8× reduction** aggressive (8 query heads → 1 KV head = MQA)\n",
    "- Scales linearly with context length (critical for 32k+ contexts)\n",
    "\n",
    "### Quality Impact:\n",
    "- **Minimal degradation** with 4-8 groups (Llama 2/3, Mistral configurations)\n",
    "- Production models show <1% perplexity increase vs MHA\n",
    "- MQA can show ~2-3% degradation but still viable\n",
    "\n",
    "### Why It Matters:\n",
    "- ✅ **Enables longer contexts** — 32k, 64k, 128k become feasible\n",
    "- ✅ **Reduces deployment cost** — fit larger models on smaller GPUs\n",
    "- ✅ **Improves throughput** — less memory movement during decoding\n",
    "- ✅ **Industry standard** — used by Llama, Mistral, Qwen, and others\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation Checklist\n",
    "\n",
    "To add GQA to your model:\n",
    "1. ✅ Add `num_kv_heads` parameter to `MultiHeadAttention`\n",
    "2. ✅ Split QKV projection into separate Q, K, V projections\n",
    "3. ✅ Implement `_repeat_kv()` helper for KV expansion\n",
    "4. ✅ Update cache shapes to use `num_kv_heads`\n",
    "5. ✅ Pass `num_kv_heads` through `TransformerBlock` and `Decoder`\n",
    "6. ✅ Test: cached generation matches full forward\n",
    "7. ✅ Verify: cache memory is reduced by expected factor\n",
    "\n",
    "**Next Steps:**\n",
    "- Train a model with GQA and compare perplexity to MHA baseline\n",
    "- Benchmark inference speed and memory usage\n",
    "- Try different `num_kv_heads` values to find optimal trade-off for your use case"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
