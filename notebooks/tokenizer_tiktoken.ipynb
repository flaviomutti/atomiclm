{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaba028-000d-4a57-9622-8482e32ee6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uv sync --extra tiktoken\n",
    "import tiktoken\n",
    "\n",
    "from atomiclm.tokenizer import BasicTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfg-cell-0001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config ---\n",
    "DATA_PATH = '../data/the-verdict.txt'   # UTF-8 text file to train on (not in repo)\n",
    "OUTPUT_PATH = '../out/vocab_tiktoken'   # save() appends .json automatically\n",
    "VOCAB_SIZE = 4096                       # target vocabulary size (base 256 + merges)\n",
    "# 100257 matches tiktoken's GPT-4 ID for <|endoftext|> — required for a compatible export.\n",
    "SPECIAL_TOKENS = {'<|endoftext|>': 100257}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-load-0001",
   "metadata": {},
   "source": [
    "## Load\n",
    "Read the raw text corpus used to learn merge rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4f860c-06d5-4f4c-b961-423e5dbf220d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f'Corpus size: {len(text):,} chars')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-train-0001",
   "metadata": {},
   "source": [
    "## Train\n",
    "Run BPE to learn merge rules. Special tokens are registered after training — they are never split by the BPE algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c64ae0e-6e17-4612-a55e-979075864e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BasicTokenizer()\n",
    "tokenizer.train(text, VOCAB_SIZE)\n",
    "tokenizer.register_special_tokens(SPECIAL_TOKENS)\n",
    "\n",
    "print(f'Learned {len(tokenizer.merges)} merge rules, vocab size: {len(tokenizer.vocab)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-save-0001",
   "metadata": {},
   "source": [
    "## Save\n",
    "Persist the trained tokenizer so it can be reloaded without retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-cell-0001",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(OUTPUT_PATH)\n",
    "print(f'Saved to {OUTPUT_PATH}.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-reload-0001",
   "metadata": {},
   "source": [
    "## Reload\n",
    "Load the tokenizer from disk and confirm the vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086421fc-4ab5-48da-b3f4-63e745a821f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BasicTokenizer()\n",
    "tokenizer.load(f'{OUTPUT_PATH}.json')\n",
    "\n",
    "print(f'Vocab size: {len(tokenizer.vocab)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-inspect-0001",
   "metadata": {},
   "source": [
    "## Inspect\n",
    "`export_mergeable_ranks()` yields `(bytes, rank)` pairs in merge-priority order — the format tiktoken expects.\n",
    "The regex `pattern` and `special_tokens` are passed through unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b60496-9476-4b50-be72-e913b49625d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mergeable_ranks = dict(tokenizer.export_mergeable_ranks())\n",
    "\n",
    "print(f'pattern        : {tokenizer.pattern}')\n",
    "print(f'mergeable_ranks: {list(mergeable_ranks.items())[-3:]}')  # last 3 as a sample\n",
    "print(f'special_tokens : {tokenizer.special_tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-export-0001",
   "metadata": {},
   "source": [
    "## Export to tiktoken\n",
    "Wrap the tokenizer's merge rules inside a `tiktoken.Encoding` object.\n",
    "After this, `enc` is a drop-in replacement for any tiktoken encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1252e64c-08f1-4515-9d9a-88b4aa93f7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.Encoding(\n",
    "    name='my_tokenizer',\n",
    "    pat_str=tokenizer.pattern,\n",
    "    mergeable_ranks=mergeable_ranks,\n",
    "    special_tokens=tokenizer.special_tokens,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-compare-0001",
   "metadata": {},
   "source": [
    "## Compare\n",
    "Encode the same string with both implementations. The merge tables are identical and both\n",
    "algorithms apply merges in rank order (lowest rank first), so the output should match.\n",
    "tiktoken uses a compiled Rust backend, so it is significantly faster at inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-cell-0001",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = 'hello world picture'\n",
    "\n",
    "ids_py  = tokenizer.encode(test_text)  # Python BPE implementation\n",
    "ids_tkt = enc.encode(test_text)        # tiktoken Rust backend\n",
    "\n",
    "print('BasicTokenizer :', ids_py)\n",
    "print('tiktoken       :', ids_tkt)\n",
    "assert ids_py == ids_tkt, 'Encoding mismatch'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
