{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6635558b-ef71-414d-a7aa-c00ac1a2404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from atomiclm.tokenizer import BasicTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfg-cell-0001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config ---\n",
    "# All paths and hyperparameters live here. Change these before running.\n",
    "DATA_PATH = '../data/the-verdict.txt'  # UTF-8 text file to train on (not in repo)\n",
    "OUTPUT_PATH = '../out/vocab'           # save() appends .json automatically\n",
    "VOCAB_SIZE = 256 + 50                  # target vocabulary size (base 256 + merges)\n",
    "SPECIAL_TOKENS = {'<|endoftext|>': VOCAB_SIZE}  # special tokens mapped to fixed IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-load-0001",
   "metadata": {},
   "source": [
    "## Load\n",
    "Read the raw text corpus. BPE operates on the full string — larger corpora produce more representative merge rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f2c01d-b19a-4527-94ed-12ce9bc11d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f'Corpus size: {len(text):,} chars')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-train-0001",
   "metadata": {},
   "source": [
    "## Train\n",
    "BPE starts with 256 byte tokens and greedily merges the most frequent adjacent pair until `VOCAB_SIZE` is reached.\n",
    "Each merge is stored in `tokenizer.merges` and used during encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-cell-0001",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BasicTokenizer()\n",
    "\n",
    "# Special tokens bypass BPE — they are matched literally before any merge is applied.\n",
    "tokenizer.register_special_tokens(SPECIAL_TOKENS)\n",
    "\n",
    "# Runs the heap-based O(n log n) BPE training algorithm.\n",
    "tokenizer.train(text, VOCAB_SIZE, verbose=True)\n",
    "\n",
    "print(f'Learned {len(tokenizer.merges)} merge rules')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-encode-0001",
   "metadata": {},
   "source": [
    "## Encode / Decode\n",
    "Encoding applies merges in rank order (highest-priority first) to produce token IDs.\n",
    "Decoding maps each ID back to its byte sequence and joins them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7405a88-c928-4a55-970b-2b7e76001095",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = 'hello world'\n",
    "\n",
    "token_ids = tokenizer.encode(test_text)\n",
    "roundtrip = tokenizer.decode(token_ids)\n",
    "\n",
    "print('token IDs :', token_ids)\n",
    "print('decoded   :', roundtrip)\n",
    "assert roundtrip == test_text, 'Roundtrip failed'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-save-0001",
   "metadata": {},
   "source": [
    "## Save\n",
    "Persists merges and special tokens to a JSON file. The file stores each merge as a `[left, right, id]` triple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cca101-af2d-4a29-8d63-7c9daeb77701",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(OUTPUT_PATH)\n",
    "print(f'Saved to {OUTPUT_PATH}.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-verify-0001",
   "metadata": {},
   "source": [
    "## Verify\n",
    "Load the saved file into a fresh tokenizer and confirm the merge rules are identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0c76b9-82e4-41f0-b39b-d1d7047f53c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded = BasicTokenizer()\n",
    "reloaded.load(f'{OUTPUT_PATH}.json')\n",
    "\n",
    "assert reloaded.merges == tokenizer.merges, 'Merge rules do not match after reload'\n",
    "print('OK — merge rules match')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
